{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh0u58VXvCZs",
        "outputId": "4f3fcd55-eca8-4368-b87a-92124b15dc71"
      },
      "id": "Gh0u58VXvCZs",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "be096f01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be096f01",
        "outputId": "c144d8e3-39d5-4292-ac5c-8adf835af823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Keras-Preprocessing in /usr/local/lib/python3.8/dist-packages (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from Keras-Preprocessing) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from Keras-Preprocessing) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "#Import libraries\n",
        "!pip install nltk\n",
        "!pip install Keras-Preprocessing\n",
        "from keras.utils import to_categorical\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from pickle import load,dump,load\n",
        "from numpy.random import shuffle\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import string\n",
        "import re\n",
        "from unicodedata import normalize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "929c1a11",
      "metadata": {
        "id": "929c1a11"
      },
      "outputs": [],
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4aec4d39",
      "metadata": {
        "id": "4aec4d39"
      },
      "outputs": [],
      "source": [
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "    lines = doc.strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in lines]\n",
        "    return pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5a0bfd5d",
      "metadata": {
        "id": "5a0bfd5d"
      },
      "outputs": [],
      "source": [
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "    cleaned = list()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    for pair in lines:\n",
        "        clean_pair = list()\n",
        "        for line in pair:\n",
        "            # normalize unicode characters\n",
        "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "            line = line.decode('UTF-8')\n",
        "            # tokenize on white space\n",
        "            line = line.split()\n",
        "            # convert to lowercase\n",
        "            line = [word.lower() for word in line]\n",
        "            # remove punctuation from each token\n",
        "            line = [re_punc.sub('', w) for w in line]\n",
        "            # remove non-printable chars form each token\n",
        "            line = [re_print.sub('', w) for w in line]\n",
        "            # remove tokens with numbers in them\n",
        "            line = [word for word in line if word.isalpha()]\n",
        "            # store as string\n",
        "            clean_pair.append(' '.join(line))\n",
        "        cleaned.append(clean_pair)\n",
        "    return array(cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "be068187",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be068187",
        "outputId": "38ec8636-6fbc-4c6c-a689-7eaf6976d131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/Machine_Translation/english-german.pkl\n",
            "[go] => [geh]\n",
            "[hi] => [hallo]\n",
            "[hi] => [gru gott]\n",
            "[run] => [lauf]\n",
            "[run] => [lauf]\n",
            "[wow] => [potzdonner]\n",
            "[wow] => [donnerwetter]\n",
            "[duck] => [kopf runter]\n",
            "[fire] => [feuer]\n",
            "[help] => [hilfe]\n",
            "[help] => [zu hulf]\n",
            "[stay] => [bleib]\n",
            "[stop] => [stopp]\n",
            "[stop] => [anhalten]\n",
            "[wait] => [warte]\n",
            "[wait] => [warte]\n",
            "[begin] => [fang an]\n",
            "[do it] => [mache es]\n",
            "[do it] => [tue es]\n",
            "[go on] => [mach weiter]\n",
            "[hello] => [hallo]\n",
            "[hello] => [sers]\n",
            "[hurry] => [beeil dich]\n",
            "[hurry] => [schnell]\n",
            "[i hid] => [ich versteckte mich]\n",
            "[i hid] => [ich habe mich versteckt]\n",
            "[i ran] => [ich rannte]\n",
            "[i see] => [ich verstehe]\n",
            "[i see] => [aha]\n",
            "[i try] => [ich versuche es]\n",
            "[i try] => [ich probiere es]\n",
            "[i won] => [ich hab gewonnen]\n",
            "[i won] => [ich habe gewonnen]\n",
            "[i won] => [ich habe gewonnen]\n",
            "[oh no] => [oh nein]\n",
            "[relax] => [entspann dich]\n",
            "[shoot] => [feuer]\n",
            "[shoot] => [schie]\n",
            "[smile] => [lacheln]\n",
            "[sorry] => [entschuldigung]\n",
            "[ask me] => [frag mich]\n",
            "[ask me] => [fragt mich]\n",
            "[ask me] => [fragen sie mich]\n",
            "[attack] => [angriff]\n",
            "[attack] => [attacke]\n",
            "[buy it] => [kaufs]\n",
            "[cheers] => [zum wohl]\n",
            "[eat it] => [iss es]\n",
            "[eat up] => [iss fertig]\n",
            "[eat up] => [iss auf]\n",
            "[eat up] => [iss auf]\n",
            "[freeze] => [keine bewegung]\n",
            "[freeze] => [stehenbleiben]\n",
            "[go now] => [geh jetzt]\n",
            "[got it] => [verstanden]\n",
            "[got it] => [ich habs]\n",
            "[got it] => [aha]\n",
            "[got it] => [kapiert]\n",
            "[got it] => [verstanden]\n",
            "[got it] => [einverstanden]\n",
            "[he ran] => [er rannte]\n",
            "[he ran] => [er lief]\n",
            "[hop in] => [mach mit]\n",
            "[hop in] => [spring rein]\n",
            "[hug me] => [druck mich]\n",
            "[hug me] => [nimm mich in den arm]\n",
            "[hug me] => [umarme mich]\n",
            "[i care] => [mir ist es wichtig]\n",
            "[i fell] => [ich fiel]\n",
            "[i fell] => [ich fiel hin]\n",
            "[i fell] => [ich sturzte]\n",
            "[i fell] => [ich bin hingefallen]\n",
            "[i fell] => [ich bin gesturzt]\n",
            "[i fled] => [ich fluchtete]\n",
            "[i fled] => [ich bin gefluchtet]\n",
            "[i know] => [ich wei]\n",
            "[i lied] => [ich habe gelogen]\n",
            "[i lost] => [ich habe verloren]\n",
            "[i paid] => [ich habe bezahlt]\n",
            "[i paid] => [ich zahlte]\n",
            "[i sang] => [ich sang]\n",
            "[i spit] => [ich spuckte]\n",
            "[i spit] => [ich habe gespuckt]\n",
            "[i swim] => [ich schwimme]\n",
            "[i wept] => [ich weinte]\n",
            "[i wept] => [ich habe geweint]\n",
            "[im] => [ich bin jahre alt]\n",
            "[im] => [ich bin]\n",
            "[im ok] => [mir gehts gut]\n",
            "[im ok] => [es geht mir gut]\n",
            "[im up] => [ich bin wach]\n",
            "[im up] => [ich bin auf]\n",
            "[listen] => [hort zu]\n",
            "[no way] => [unmoglich]\n",
            "[no way] => [das kommt nicht in frage]\n",
            "[no way] => [das gibts doch nicht]\n",
            "[no way] => [ausgeschlossen]\n",
            "[no way] => [in keinster weise]\n",
            "[really] => [wirklich]\n",
            "[really] => [echt]\n"
          ]
        }
      ],
      "source": [
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print('Saved: %s' % filename)\n",
        "# load dataset\n",
        "filename = '/content/drive/MyDrive/Machine_Translation/deu.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, '/content/drive/MyDrive/Machine_Translation/english-german.pkl')\n",
        "# spot check\n",
        "for i in range(10):\n",
        "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6d3221bb",
      "metadata": {
        "id": "6d3221bb"
      },
      "outputs": [],
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('/content/drive/MyDrive/Machine_Translation/english-german.pkl')\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[9000:]\n",
        "# save\n",
        "save_clean_data(dataset, '/content/drive/MyDrive/Machine_Translation/english-german-both.pkl')\n",
        "save_clean_data(train, '/content/drive/MyDrive/Machine_Translation/english-german-train.pkl')\n",
        "save_clean_data(test, '/content/drive/MyDrive/Machine_Translation/english-german-test.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-utHMzXvxyQD",
        "outputId": "bc4da888-ffbe-4fe6-e210-37d90dfd286a"
      },
      "id": "-utHMzXvxyQD",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/Machine_Translation/english-german-both.pkl\n",
            "Saved: /content/drive/MyDrive/Machine_Translation/english-german-train.pkl\n",
            "Saved: /content/drive/MyDrive/Machine_Translation/english-german-test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "64690206",
      "metadata": {
        "id": "64690206"
      },
      "outputs": [],
      "source": [
        "# load datasets\n",
        "dataset = load_clean_sentences('/content/drive/MyDrive/Machine_Translation/english-german-both.pkl')\n",
        "train = load_clean_sentences('/content/drive/MyDrive/Machine_Translation/english-german-train.pkl')\n",
        "test = load_clean_sentences('/content/drive/MyDrive/Machine_Translation/english-german-test.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "56eed4fc",
      "metadata": {
        "id": "56eed4fc"
      },
      "outputs": [],
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a1d71c6b",
      "metadata": {
        "id": "a1d71c6b"
      },
      "outputs": [],
      "source": [
        "# max sentence length\n",
        "def max_length(lines):\n",
        "    return max(len(line.split()) for line in lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f0723cff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0723cff",
        "outputId": "e46f0e59-4029-46ac-f383-ebeaab93bb09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 2176\n",
            "English Max Length: 5\n"
          ]
        }
      ],
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a38abdcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a38abdcb",
        "outputId": "7735532a-7b21-4229-f42d-415ea780d054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "German Vocabulary Size: 3534\n",
            "German Max Length: 9\n"
          ]
        }
      ],
      "source": [
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('German Max Length: %d' % (ger_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4aa66867",
      "metadata": {
        "id": "4aa66867"
      },
      "outputs": [],
      "source": [
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # integer encode sequences\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    X = pad_sequences(X, maxlen=length, padding='post')\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4be68de9",
      "metadata": {
        "id": "4be68de9"
      },
      "outputs": [],
      "source": [
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "    ylist = list()\n",
        "    for sequence in sequences:\n",
        "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "        ylist.append(encoded)\n",
        "    y = array(ylist)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "7a131996",
      "metadata": {
        "id": "7a131996"
      },
      "outputs": [],
      "source": [
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "4d3bc02d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d3bc02d",
        "outputId": "c4bed503-2c79-4245-8435-1f4326508de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 9, 256)            904704    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 256)               525312    \n",
            "                                                                 \n",
            " repeat_vector (RepeatVector  (None, 5, 256)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 5, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 5, 2176)          559232    \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,514,560\n",
            "Trainable params: 2,514,560\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "    model.add(LSTM(n_units))\n",
        "    model.add(RepeatVector(tar_timesteps))\n",
        "    model.add(LSTM(n_units, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "    # compile model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "5021f1f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5021f1f3",
        "outputId": "0ef15fd5-129c-4bff-b0b6-4d9fa10d5145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "141/141 - 44s - loss: 4.0883 - val_loss: 3.3400 - 44s/epoch - 313ms/step\n",
            "Epoch 2/60\n",
            "141/141 - 27s - loss: 3.1912 - val_loss: 3.1917 - 27s/epoch - 192ms/step\n",
            "Epoch 3/60\n",
            "141/141 - 26s - loss: 3.0365 - val_loss: 3.0859 - 26s/epoch - 181ms/step\n",
            "Epoch 4/60\n",
            "141/141 - 25s - loss: 2.8775 - val_loss: 2.9777 - 25s/epoch - 180ms/step\n",
            "Epoch 5/60\n",
            "141/141 - 30s - loss: 2.7372 - val_loss: 2.8749 - 30s/epoch - 211ms/step\n",
            "Epoch 6/60\n",
            "141/141 - 26s - loss: 2.5853 - val_loss: 2.7812 - 26s/epoch - 184ms/step\n",
            "Epoch 7/60\n",
            "141/141 - 26s - loss: 2.4277 - val_loss: 2.6481 - 26s/epoch - 186ms/step\n",
            "Epoch 8/60\n",
            "141/141 - 26s - loss: 2.2820 - val_loss: 2.5520 - 26s/epoch - 184ms/step\n",
            "Epoch 9/60\n",
            "141/141 - 26s - loss: 2.1301 - val_loss: 2.4606 - 26s/epoch - 185ms/step\n",
            "Epoch 10/60\n",
            "141/141 - 26s - loss: 1.9923 - val_loss: 2.3643 - 26s/epoch - 183ms/step\n",
            "Epoch 11/60\n",
            "141/141 - 29s - loss: 1.8670 - val_loss: 2.2949 - 29s/epoch - 206ms/step\n",
            "Epoch 12/60\n",
            "141/141 - 26s - loss: 1.7521 - val_loss: 2.2371 - 26s/epoch - 182ms/step\n",
            "Epoch 13/60\n",
            "141/141 - 25s - loss: 1.6461 - val_loss: 2.1845 - 25s/epoch - 175ms/step\n",
            "Epoch 14/60\n",
            "141/141 - 26s - loss: 1.5437 - val_loss: 2.1566 - 26s/epoch - 186ms/step\n",
            "Epoch 15/60\n",
            "141/141 - 26s - loss: 1.4510 - val_loss: 2.0862 - 26s/epoch - 186ms/step\n",
            "Epoch 16/60\n",
            "141/141 - 27s - loss: 1.3582 - val_loss: 2.0509 - 27s/epoch - 188ms/step\n",
            "Epoch 17/60\n",
            "141/141 - 26s - loss: 1.2701 - val_loss: 2.0135 - 26s/epoch - 186ms/step\n",
            "Epoch 18/60\n",
            "141/141 - 26s - loss: 1.1846 - val_loss: 1.9827 - 26s/epoch - 183ms/step\n",
            "Epoch 19/60\n",
            "141/141 - 25s - loss: 1.1012 - val_loss: 1.9485 - 25s/epoch - 175ms/step\n",
            "Epoch 20/60\n",
            "141/141 - 25s - loss: 1.0211 - val_loss: 1.9218 - 25s/epoch - 176ms/step\n",
            "Epoch 21/60\n",
            "141/141 - 26s - loss: 0.9437 - val_loss: 1.8921 - 26s/epoch - 185ms/step\n",
            "Epoch 22/60\n",
            "141/141 - 26s - loss: 0.8701 - val_loss: 1.8784 - 26s/epoch - 185ms/step\n",
            "Epoch 23/60\n",
            "141/141 - 26s - loss: 0.7986 - val_loss: 1.8501 - 26s/epoch - 182ms/step\n",
            "Epoch 24/60\n",
            "141/141 - 26s - loss: 0.7342 - val_loss: 1.8337 - 26s/epoch - 185ms/step\n",
            "Epoch 25/60\n",
            "141/141 - 26s - loss: 0.6733 - val_loss: 1.8288 - 26s/epoch - 181ms/step\n",
            "Epoch 26/60\n",
            "141/141 - 25s - loss: 0.6172 - val_loss: 1.8193 - 25s/epoch - 174ms/step\n",
            "Epoch 27/60\n",
            "141/141 - 26s - loss: 0.5643 - val_loss: 1.8061 - 26s/epoch - 185ms/step\n",
            "Epoch 28/60\n",
            "141/141 - 25s - loss: 0.5167 - val_loss: 1.8166 - 25s/epoch - 179ms/step\n",
            "Epoch 29/60\n",
            "141/141 - 26s - loss: 0.4726 - val_loss: 1.7845 - 26s/epoch - 187ms/step\n",
            "Epoch 30/60\n",
            "141/141 - 25s - loss: 0.4320 - val_loss: 1.7985 - 25s/epoch - 177ms/step\n",
            "Epoch 31/60\n",
            "141/141 - 23s - loss: 0.4000 - val_loss: 1.7936 - 23s/epoch - 164ms/step\n",
            "Epoch 32/60\n",
            "141/141 - 25s - loss: 0.3665 - val_loss: 1.7932 - 25s/epoch - 174ms/step\n",
            "Epoch 33/60\n",
            "141/141 - 25s - loss: 0.3351 - val_loss: 1.7926 - 25s/epoch - 175ms/step\n",
            "Epoch 34/60\n",
            "141/141 - 25s - loss: 0.3101 - val_loss: 1.7907 - 25s/epoch - 175ms/step\n",
            "Epoch 35/60\n",
            "141/141 - 26s - loss: 0.2863 - val_loss: 1.7952 - 26s/epoch - 184ms/step\n",
            "Epoch 36/60\n",
            "141/141 - 24s - loss: 0.2633 - val_loss: 1.8061 - 24s/epoch - 171ms/step\n",
            "Epoch 37/60\n",
            "141/141 - 25s - loss: 0.2455 - val_loss: 1.8111 - 25s/epoch - 178ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8284011550>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# fit model\n",
        "from keras.callbacks import EarlyStopping\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/Machine_Translation/model.h5', monitor='val_loss',save_best_only=True, mode='min')\n",
        "callback = EarlyStopping(monitor='val_loss', patience=8)\n",
        "model.fit(trainX, trainY, epochs=60, batch_size=64, validation_data=(testX, testY),callbacks=[checkpoint,callback], verbose=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "735c9dff",
      "metadata": {
        "id": "735c9dff"
      },
      "outputs": [],
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "866773ba",
      "metadata": {
        "id": "866773ba"
      },
      "outputs": [],
      "source": [
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "8ab28d4d",
      "metadata": {
        "id": "8ab28d4d"
      },
      "outputs": [],
      "source": [
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target= raw_dataset[i][0]\n",
        "\t\traw_src = raw_dataset[i][1]\n",
        "\t\tif i < 20:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "a63ab1de",
      "metadata": {
        "id": "a63ab1de"
      },
      "outputs": [],
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "0b7efc29",
      "metadata": {
        "id": "0b7efc29"
      },
      "outputs": [],
      "source": [
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "a92f8285",
      "metadata": {
        "id": "a92f8285"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "model = load_model('/content/drive/MyDrive/Machine_Translation/model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "366c5eac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "366c5eac",
        "outputId": "cbd4baa5-7c57-4c99-dc06-13a6de802f6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "src=[das gehort tom], target=[its toms], predicted=[thats toms]\n",
            "src=[er hat mir den laufpass gegeben], target=[he dumped me], predicted=[he dumped me]\n",
            "src=[tom wei es jetzt], target=[tom knows now], predicted=[tom knows it]\n",
            "src=[ich rieche kaffee], target=[i smell coffee], predicted=[i smell coffee]\n",
            "src=[du bist zuruck], target=[youre back], predicted=[youre back]\n",
            "src=[uns ist hei], target=[were hot], predicted=[were hot]\n",
            "src=[scham dich], target=[shame on you], predicted=[shame on you]\n",
            "src=[es ist geschehen], target=[its happened], predicted=[its happened]\n",
            "src=[ich habe eine arbeit], target=[ive got a job], predicted=[i have a job]\n",
            "src=[frag einen lehrer], target=[ask a teacher], predicted=[ask a teacher]\n",
            "src=[ich habe noch mehr], target=[ive got more], predicted=[i got more]\n",
            "src=[tom ist fair], target=[tom is fair], predicted=[tom is fair]\n",
            "src=[komm nicht zu spat], target=[dont be late], predicted=[dont be late]\n",
            "src=[atme aus], target=[breathe out], predicted=[breathe]\n",
            "src=[er sieht gut aus], target=[he looks well], predicted=[he looks well]\n",
            "src=[tom gibt uns die schuld], target=[tom blames us], predicted=[tom blames us]\n",
            "src=[ich summe], target=[im humming], predicted=[im humming]\n",
            "src=[ich umarmte tom], target=[i hugged tom], predicted=[i hugged tom]\n",
            "src=[ich war naiv], target=[i was naive], predicted=[i was naive]\n",
            "src=[wir haben pech], target=[were unlucky], predicted=[were unlucky]\n",
            "src=[bitte tu es], target=[please do it], predicted=[please do it]\n",
            "src=[ich bin in einer gang], target=[im in a gang], predicted=[im in a in]\n",
            "src=[ich habe angst], target=[im afraid], predicted=[im afraid]\n",
            "src=[wann gibts abendbrot], target=[whens dinner], predicted=[whens dinner]\n",
            "src=[tom fuhlte sich gut], target=[tom felt fine], predicted=[tom felt well]\n",
            "BLEU-1: 0.854601\n",
            "BLEU-2: 0.795362\n",
            "BLEU-3: 0.684160\n",
            "BLEU-4: 0.359884\n"
          ]
        }
      ],
      "source": [
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "32e24f86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32e24f86",
        "outputId": "e1a900dc-979c-4300-f46a-dd4001992990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n",
            "src=[ich habe ihnen geglaubt], target=[i believed you], predicted=[i believed you]\n",
            "src=[es ist warm hier], target=[its warm here], predicted=[its here]\n",
            "src=[nimm nur eine], target=[take only one], predicted=[take only one]\n",
            "src=[die glocke lautete], target=[the bell rang], predicted=[youre cry]\n",
            "src=[ich habe kein talent], target=[im untalented], predicted=[i ate no]\n",
            "src=[sie sind bescheiden], target=[youre modest], predicted=[youre modest]\n",
            "src=[ich bin nass], target=[im wet], predicted=[im am]\n",
            "src=[macht ein nickerchen], target=[take a nap], predicted=[take a nap]\n",
            "src=[ist ihnen nicht hei], target=[arent you hot], predicted=[arent you hot]\n",
            "src=[uberrascht mich], target=[surprise me], predicted=[me me]\n",
            "src=[das ist in ordnung], target=[this is ok], predicted=[thats is]\n",
            "src=[tom ist auf], target=[toms up], predicted=[tom is tidy]\n",
            "src=[ist das nicht cool], target=[isnt it cool], predicted=[is it easy]\n",
            "src=[gib mir eine minute], target=[wait a minute], predicted=[give a a]\n",
            "src=[aber sicher doch], target=[of course], predicted=[dry are eyes]\n",
            "src=[ist das legal], target=[is this legal], predicted=[is that legal]\n",
            "src=[ich bin hier beschaftigt], target=[im busy here], predicted=[im busy here]\n",
            "src=[wir kennen tom], target=[we know tom], predicted=[we know tom]\n",
            "src=[das ist ihre aufgabe], target=[its their job], predicted=[this is cat]\n",
            "src=[ich bin sicher], target=[im positive], predicted=[im very]\n",
            "src=[ich zeige es ihnen], target=[ill show you], predicted=[i loved it]\n",
            "src=[ich bin feuerwehrmann], target=[im a fireman], predicted=[im sincere]\n",
            "src=[ich habe arbeit], target=[i have a job], predicted=[i have wine]\n",
            "src=[tom macht mir angst], target=[tom scares me], predicted=[tom will me]\n",
            "src=[tom hatte spa], target=[tom had fun], predicted=[tom was fun]\n",
            "BLEU-1: 0.537282\n",
            "BLEU-2: 0.404359\n",
            "BLEU-3: 0.311696\n",
            "BLEU-4: 0.129442\n"
          ]
        }
      ],
      "source": [
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7831cdff",
      "metadata": {
        "id": "7831cdff"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}